---
title: "MATP-4400 COVID-19 Final Notebook"
author: "Daniel Tabin"
date: "May 6 2020"
output:
  html_document:
    number_sections: yes
    toc: yes
    toc_depth: 2
  pdf_document:
    toc: yes
    toc_depth: '2'
---

```{r setup, include=FALSE}
# stick libraries here
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)

if (!require("ggplot2")) {
  install.packages("ggplot2")
  library(ggplot2)
}

if (!require("knitr")) {
  install.packages("knitr")
  library(knitr)
}

if (!require("matlab")) {
   install.packages("matlab")
   library(matlab)
}

if (!require("anchors")) {
   install.packages("anchors")
   library(anchors)
}


if (!require("ggbiplot")) {
   devtools::install_git("https://github.com/vqv/ggbiplot.git")
   library(ggbiplot)
}

if (!require("scatterplot3d")) {
  install.packages("scatterplot3d")
  library(scatterplot3d)
}

if (!require("gplots")) {
  install.packages("gplots")
  library(gplots)
}

if (!require('fields')) {
  install.packages("fields")
  library(fields)
}

if (!require('dplyr')) {
  install.packages("dplyr")
  library(dplyr)
}

if (!require('gdata')) {
  install.packages("gdata")
  library(gdata)
}

if (!require('leaflet')) {
  install.packages("leaflet")
  library(leaflet)
}

knitr::opts_chunk$set(echo = TRUE)

```

# Final Project: Guidance for Students (DELETE)

*DELETE THIS SECTION IN FINAL NOTEBOOK!*

##GENERAL INSTRUCTIONS (DELETE)##

* This notebook serves as a written presentation of your work so it should be written like a research paper.
* The rubric for evaluating this project is on LMS.
* The R code that executes the results should be embedded in this notebook if possible. It's also okay to "source" external scripts from within your notebook, including those provided by your instructors. 
* If there is work that is not appropriate to be embedded on your final notebook, then you can describe the work in the notebook and provide figures generated elsewhere (e.g. screen shots, graphs). Indicate if that work has been committed to github. If necessary put details in Appendix.    
* Your wording should be suitable for sharing with external partners/mentors and useful to future contributors.
* Please don't summarize everything you did; discuss only the *most important* aspects of your work. Ask yourself *what really matters?*
* IMPORTANT: Discuss any insights you found regarding COVID-19.
* If there are limitations to your work, discuss. 
* Include any background or support for your work. For example, mention any relevant research articles you found -- and be sure to include references!
* Include your recommendations for extending the COVIDMINDER app:
    + Data utilization...
    + Analytics...
    + Visualizations
    + User interface design...
* **Expect internet problems!** Commit versions of this notebook to the IDEA github *early and often*. You can continue to refine and update; that is the point of github!

##SUBMISSION CHECKLIST: Things to check before you submit (DELETE)##

* Have you given your file(s) a clear, sensible name? `my_notebook.Rmd` or `joey.Rmd` are **not* acceptable!
* Is every figure clearly labeled and titled? 
* Does every figure serve a purpose? 
     + What question does it answer? 
     + You can put extra (non-essential) figues in the Appendix.
* Are your tables clearly labelled and legible?
     + We recommend using `kable()` (built into `knitr`)
     + `xtable()` might also work for you (not as easy as `kable()`)
* **CRITICAL:** Have you given enough information for someone to reproduce, understand and extend your results?
     + Where can they *find* the data that you used?
     + Have you *described* the data that used?
     + Have you *documented* your code? 
     + Are your figures *clearly labeled*?
     + Did you *discuss your findings*?
     + Did you use good grammar and *proofread* your results?
     + Finally, have you *committed* your work to github and made a *pull request*?
     
## github Submission Instructions (DELETE!)

Fill out the bullets at the beginning of this notebook...

* Provide your github ID. 
* Indicate the issues associated with this work on github. 
* Commit the Rmd and html versions of this notebook to the `MATP-4400-FINAL` directory on github and make a pull request.  
* Provide the link to your github submission on LMS. 
    + **NOTE:** You can post your link any time after it exists  
    + We will grade the latest version available on Github starting at 3:00 on Thursday, 7 May.  
* **NOTE:** RStudio Server will be unavailable due to maintenance for up to a week beginning the weekend of 9 May,  so there can be no extensions or late submissions. 

##github help (DELETE!)

* DO THIS FIRST:
    + `git clone https://github.com/TheRensselaerIDEA/COVID-Notebooks` (if you haven't already)
        + NOTE: We may have added to the master since your last `git pull` so make sure to do a `git pull origin master`
    + `git checkout -b <your-new-branch>` (more instructions immediately below)
    + In R, create a new R notebook (ie a `.Rmd` file) in the subdirectory `MATP-4400-FINAL`
    + `git add` it to the repo (make sure you're in the correct directory when you do this!)
    + `git commit -a -m 'Created my notebook!'` as soon as you do this! 
    + `git push origin <your-new-branch>` so we see you're working on something
    + Post a link in LMS; it will remain valid as you update (and check in) your work to github
    + for details, see instructions in next few bullets
    + Contact John Erickson erickj4@rpi.edu for further github questions
* See: **HOW TO USE THIS NOTEBOOK AND REPOSITORY** https://bit.ly/2wYQGXP
* See also: **git Cheat Sheet** https://bit.ly/2yCBSi8 

*DELETE THE SECTIONS ABOVE!*
<hr>

# Final Project: Submission Links

*This should be the first section of your final project notebook. Fill out the following according to how you submitted your notebook!*

* github repository: https://github.com/TheRensselaerIDEA/COVID-Notebooks/tree/state-report-card-daniel-tabin-project 
* My github ID: *Darokrithia*
* github issues addressed by this work: `#31` (example)
* Github branch name of my submitted notebook: *state-report-card-daniel-tabin-project* (example)
* link to merged notebook (post these to LMS!: 
    + https://github.com/TheRensselaerIDEA/COVID-Notebooks/blob/state-report-card-daniel-tabin-project/MATP-4400-FINAL/COVID_FINAL_2020_Daniel_Tabin.Rmd (example; Rmd version)
    + https://github.com/TheRensselaerIDEA/COVID-Notebooks/blob/state-report-card-daniel-tabin-project/MATP-4400-FINAL/COVID_FINAL_2020_Daniel_Tabin.html (example; HTML version)

# Overview & Problems Tackled

*Provide a top-level summary of your work and findings.*

The overarching goal of my project was to make a sort of report card for every state, so we can see how well said state is doing during the corona crisis.  I had a couple of personal requirements for this report card.  For example, I want the number to be indpenedent of how large a state is, when the outbreak started in a state, and how the state has done in the past.  I split up this problem into two seperate sub problems.

First I tried to use PCA's and heatmaps to determine the relationship between the rate of number of risk factors and the rate COVID cases and COVID deaths in a state.  I tried a number of different combinations, as discussed bellow, but overall I was unable to see a strong and satisfactory correlation between the rates any of the factors and the rates of COVID spread and deaths on a state wide level.  This suggests that while prexisting condtions are highly important for an individual's survival, there are other factors that are more important for survival rates, such as social distancing and shutting down non-essential services.

After my first problem was brought to completion, I realized that I would not be able to use the data I already had to generate some kind of formula for a grade.  As such I figured that I would develop a new way of looking at the data I had beyond just giving the raw numbers of deaths and new cases.  Instead I figured out how to generate the percent of cases that occured in the last week, and then used that to make maps that could be used for grading.

# Data Description

*Include data sources/locations, versions/dates, etc.* 

Due to the fact that I was analysing multiple factors, I used a large number of data sets.  All of these were obtained from the /data/csv/ folder in the COVID-Notebooks repository.  These include at_risk_adults.csv, uninsured_by_state.csv, states_abbreviations.csv, provider_capacity.csv, hypertension_mortality.csv, diabetes_data_states.csv, covid_data_states.csv.  I later added the following time_series_covid19_deaths_US.csv and time_series_covid19_confirmed_US.csv when I was working on the second problem.

I used at_risk_adults.csv to get the population of adults at risk by state, uninsured_by_state.csv to get the amount of uninsured by people per state, states_abbreviations.csv to help merge the data, provider_capacity.csv to find hospital beds per 100 people by state, hypertension_mortality.csv to find heart disease data per state, diabetes_data_states.csv to find diabetes data per state, covid_data_states.csv to find covid deaths per state, time_series_covid19_deaths_US.csv to get deaths per day per US county, and time_series_covid19_confirmed_US.csv to get new covid cases per day per US county.

# Results

## Problem 1 

*Describe the problem you are examining.  If there is background that is necessary for this problem, then put it here. Include any references.* 

It is well known that certain prexisting conditions such as heard disease, obesity, diabetes, and age can put people at a much higher risk of dying.  More over a big cause of death with COVID is a lack of hospital beds and general medical access.  As such I would like to explore if the rates of any of these features are correlated with state-wide COVID spread or COVID death.  I plan on doing this by creating a combined data-frame, and then running a PCA and heatmap on said data-frame.

 
### Methods
The first thing I did was load in the data from all the csv files I found
```{r}
#load in each data set
at_risk_adults <- read.csv('../data/csv/at_risk_adults.csv')
#rownames in this specific set were bugged, so I deleted them
uninsured_by_state <- read.csv('../data/csv/uninsured_by_state.csv', row.names = NULL)
states_abbreviations<- read.csv('../data/csv/states_abbreviations.csv')
provider_capacity <- read.csv('../data/csv/provider_capacity.csv')
hypertension_mortality <- read.csv('../data/csv/hypertension_mortality.csv')
diabetes_data_states <- read.csv('../data/csv/diabetes_data_states.csv')
covid_data_states <- read.csv('../data/csv/covid_data_states.csv')

#load in data used in later half
time_series_covid19_deaths_US <- read.csv('../data/csv/time_series/time_series_covid19_deaths_US.csv')
time_series_covid19_confirmed_US <- read.csv('../data/csv/time_series/time_series_covid19_confirmed_US.csv')

```
I then combined the data into a single master data matrix
```{r}
#renamed the columns on the broken piece of data
colnames(uninsured_by_state) = c("NAME", "Status", "Employer", "Non-group", "Medicaid", "Medicare", "Military", "Uninsured", "number_uninsured")
names(diabetes_data_states)[names(diabetes_data_states) == 'State'] <- 'NAME'

#form master matrix
master_data <- merge(at_risk_adults, uninsured_by_state, by="NAME")
master_data <- merge(master_data, states_abbreviations, by="NAME", type="inner")
master_data <- merge(master_data, provider_capacity, by="NAME", type="inner")
master_data <- merge(master_data, hypertension_mortality, by="NAME", type="inner")
master_data <- merge(master_data, diabetes_data_states, by="NAME", type="inner")
master_data <- merge(master_data, covid_data_states, by="NAME", type="inner")

#rename columns so they will merge properly
names(time_series_covid19_confirmed_US)[names(time_series_covid19_confirmed_US) == 'Province_State'] <- 'NAME'
names(time_series_covid19_deaths_US)[names(time_series_covid19_deaths_US) == 'Province_State'] <- 'NAME'

#Load last day two days into matrix
latest_confirmed <- time_series_covid19_confirmed_US [ , c(7, ncol(time_series_covid19_confirmed_US), ncol(time_series_covid19_confirmed_US)-1)] 
latest_deaths <- time_series_covid19_deaths_US [ , c(7, ncol(time_series_covid19_deaths_US), ncol(time_series_covid19_deaths_US)-1)]

#rename dates into previous and total
names(latest_confirmed)[3] <- 'prev_c'
names(latest_deaths)[3] <- 'prev_d'
names(latest_confirmed)[2] <- 'total_c'
names(latest_deaths)[2] <- 'total_d'

#calculate deaths and confirmed cases in the last day
latest_deaths$daily_deaths <- (latest_deaths$total_d - latest_deaths$prev_d)
latest_confirmed$daily_confirmed <- (latest_confirmed$total_c - latest_confirmed$prev_c)

#form new matrixes
latest_deaths <- latest_deaths[, c("NAME", "daily_deaths")]
latest_confirmed <- latest_confirmed[, c("NAME", "daily_confirmed")]

#aggregatd counties into states
latest_confirmed <- aggregate(latest_confirmed['daily_confirmed'], by=latest_confirmed['NAME'], sum)
latest_deaths <- aggregate(latest_deaths['daily_deaths'], by=latest_deaths['NAME'], sum)

#add to master data
master_data <- merge(master_data, latest_confirmed, by="NAME", type="inner")
master_data <- merge(master_data, latest_deaths, by="NAME", type="inner")

```
Make "gradable" matrixes which will consist of only the data I will use
```{r}
# The "original" gradable data, with no COVID attached
gradable_data <- master_data[, c("p_at_risk_adults", "p_older_at_risk_adults", "Uninsured", "hosp_beds_per_1000", "p_ht_death_rate", "pct_Adults_with_Diabetes")]
colnames(gradable_data) = c("At risk Adults", "Older at risk Adults", "Uninsured", "Hospital Beds", "Heart Disease", "Diabetes")

# Gradable data with total covid data
gradable_data2 <- master_data[, c("p_at_risk_adults", "p_older_at_risk_adults", "Uninsured", "hosp_beds_per_1000", "p_ht_death_rate", "pct_Adults_with_Diabetes", "covid19_cases", "covid19_deaths")]
colnames(gradable_data2) = c("At risk Adults", "Older at risk Adults", "Uninsured", "Hospital Beds", "Heart Disease", "Diabetes", "Covid Cases", "Covid Deaths")

# Gradable data with just new COVID data
gradable_data3 <- master_data[, c("p_at_risk_adults", "p_older_at_risk_adults", "Uninsured", "hosp_beds_per_1000", "p_ht_death_rate", "pct_Adults_with_Diabetes", "daily_confirmed", "daily_deaths")]
colnames(gradable_data3) = c("At risk Adults", "Older at risk Adults", "Uninsured", "Hospital Beds", "Heart Disease", "Diabetes", "Covid Cases", "Covid Deaths")
```
Form kmeans PCA's and heatmaps.  I use a kmean of 5 because I want to break the states down into 5 "grades" A, B, C, D, and F.  As seen below, this did not work out exactly as planned
```{r}
set.seed(300)
#use k-means to generate clusers
km <-kmeans(scale(data.matrix(gradable_data)), 5)
km$cluster <- as.character(km$cluster)
my.pca <- prcomp(scale(data.matrix(gradable_data)), retx=TRUE, center=TRUE, scale=TRUE)
# Calculate x and y scale limits for the biplot
t<-max(abs(my.pca$x[,1:2]))
# Generate the biplot using ggbiplot
p <- ggbiplot(my.pca,
            choices=c(1,2),
            varname.adjust=0.5,
            obs.scale = 1,
            groups=as.factor(km$cluster))
p + ggtitle('First two PCs of gradable data') + xlim(-t,t) + ylim(-t,t)
p <- ggbiplot(my.pca,
            choices=c(2,3),
            varname.adjust=0.5,
            obs.scale = 1,
            groups=as.factor(km$cluster))
p + ggtitle('Second two PCs of gradable data') + xlim(-t,t) + ylim(-t,t)
heatmap.2(km$centers, 
          scale = "none",
          dendrogram = "none",
          Colv=FALSE,
          cexCol=0.5,
          main = "Heatmap of gradable features",
          trace ="none")

km2 <-kmeans(scale(data.matrix(gradable_data2)), 5)
km2$cluster <- as.character(km$cluster)
my2.pca <- prcomp(scale(data.matrix(gradable_data2)), retx=TRUE, center=TRUE, scale=TRUE)
# Calculate x and y scale limits for the biplot
t<-max(abs(my2.pca$x[,1:2]))
# Generate the biplot using ggbiplot
p <- ggbiplot(my2.pca,
            choices=c(1,2),
            varname.adjust=0.5,
            obs.scale = 1,
            groups=as.factor(km2$cluster))
p + ggtitle('First two PCs of gradable data (including COVID data)') + xlim(-t,t) + ylim(-t,t)
p <- ggbiplot(my2.pca,
            choices=c(2,3),
            varname.adjust=0.5,
            obs.scale = 1,
            groups=as.factor(km2$cluster))
p + ggtitle('Second two PCs of gradable data (including COVID data)') + xlim(-t,t) + ylim(-t,t)
heatmap.2(km2$centers, 
          scale = "none",
          dendrogram = "none",
          Colv=FALSE,
          cexCol=0.5,
          main = "Heatmap of gradable features (including COVID data)",
          trace ="none")



km3 <-kmeans(scale(data.matrix(gradable_data3)), 5)
km3$cluster <- as.character(km$cluster)
my3.pca <- prcomp(scale(data.matrix(gradable_data3)), retx=TRUE, center=TRUE, scale=TRUE)
# Calculate x and y scale limits for the biplot
t<-max(abs(my3.pca$x[,1:2]))
# Generate the biplot using ggbiplot
p <- ggbiplot(my3.pca,
            choices=c(1,2),
            varname.adjust=0.5,
            obs.scale = 1,
            groups=as.factor(km3$cluster))
p + ggtitle('First two PCs of gradable data (including latest COVID data)') + xlim(-t,t) + ylim(-t,t)
p <- ggbiplot(my3.pca,
            choices=c(2,3),
            varname.adjust=0.5,
            obs.scale = 1,
            groups=as.factor(km3$cluster))
p + ggtitle('Second two PCs of gradable data (including latest COVID data)') + xlim(-t,t) + ylim(-t,t)
heatmap.2(km3$centers, 
          scale = "none",
          dendrogram = "none",
          Colv=FALSE,
          cexCol=0.5,
          main = "Heatmap of gradable features (including latest COVID data)",
          trace ="none")
```
### Results

*What were the results on this problem?*

It can be seen that almost all features are nearly orthagonal to COVID cases and deaths.  Moreover the features themselves don't seem to be highly correlated.  Some interesting semi-correlations is the lack of older at risk adults in states with higher Diabetes and Heart disease.  This makes sense as states with worse general health will have less people who live to a long age.  As mentioned there was almost corrlation with COVID death or COVID rates, except for the very strong corelation between COVID death and COVID rates.

The lack of a strong correlation implies that what matters most for states is actions, and not underlying conditions.  I was most suprised by the lack of correlation with hospital beds and insurance.  I was fairly confident that states with less access to health care would end up with worse outcomes, but this overall does not seem to be the case.

I originally only did the first two gradable dates to try and find correlations between the factors and each other, and between the factors and COVID.  After this failed to generate the resutls I wanted, I tried generating only recent COVID death data; however, this data ended up looking strikingly similar to the total COVID data.

### Discussion

As mentioned, I feel as though the results of the first problem show that the rate prexisting conditions and access to health care do not accurately predict the severity of a COVID infection in a state.  As such I do not feel comfortable using any of these features as method of grading a state.  There are likely many factors that influence a states outcome.  These include but are not limited to how early a state was infected, how quickly a state responded, what the state did to respond, how a state's population followed the state's guidlines and reponse, and how densly populated a state is.  There may yet be other ways of grading a state, one of which will be explored in problem 2, but state-wide rates of preexisting conditions and medical access are not the way to do this.

## Problem 2
 
In this problem I wanted to create a Map showing how well a state was doing currently without using features that may or may not correlate with actual COVID outcomes.  I considered looking at data such as social distancing, but given that there was no garuentee that data would have strong correlations I decided to look at something more concrete.  I wanted something directly tied to COVID rates, indpendent of state size, independent of previous failure or sucess, and independent of infection.

### Methods
First I loaded in similar data to the third part of problem 1
```{r}
# Like in problem one, rename the columns so they play nice together
names(time_series_covid19_confirmed_US)[names(time_series_covid19_confirmed_US) == 'Province_State'] <- 'NAME'
names(time_series_covid19_deaths_US)[names(time_series_covid19_deaths_US) == 'Province_State'] <- 'NAME'

# Collect data from most recent day, and seven days before most recent day
last_week_confirmed <- time_series_covid19_confirmed_US [ , c(7, ncol(time_series_covid19_confirmed_US), ncol(time_series_covid19_confirmed_US)-7)] 
last_week_deaths <- time_series_covid19_deaths_US [ , c(7, ncol(time_series_covid19_deaths_US), ncol(time_series_covid19_deaths_US)-7)]

# Rename data
names(last_week_confirmed)[3] <- 'prev_c'
names(last_week_deaths)[3] <- 'prev_d'
names(last_week_confirmed)[2] <- 'total_c'
names(last_week_deaths)[2] <- 'total_d'

```
I then did a bunch of calculations to generate percent increases and recent changes
```{r}
# Take only relevant columns
latest_death_percent <- last_week_deaths[, c("NAME", "prev_d", "total_d")]
latest_confirmed_percent <- last_week_confirmed[, c("NAME", "prev_c", "total_c")]

# Calculate precentages
last_week_deaths$last_week_deaths <- (last_week_deaths$total_d - last_week_deaths$prev_d)
last_week_confirmed$last_week_confirmed <- (last_week_confirmed$total_c - last_week_confirmed$prev_c)
last_week_deaths$percent_deaths <- (last_week_deaths$last_week_deaths / last_week_deaths$total_d)
last_week_confirmed$percent_confirmed <- (last_week_confirmed$last_week_confirmed / last_week_confirmed$total_c)

#Removed NaNs (areas with no cases or deaths)
death_df <- last_week_deaths[complete.cases(last_week_deaths), ]
confirmed_df <- last_week_confirmed[complete.cases(last_week_confirmed), ]
```
After this I attempted to plot my Data
```{r}
# Density plots
ggplot(confirmed_df) +
  geom_density(aes(x = percent_confirmed, color = NAME)) +
  ggtitle("Distribution of confirmed growth per county")

ggplot(death_df) +
  geom_density(aes(x = percent_deaths, color = NAME)) +
  ggtitle("Distribution of death growth per county")

# Box plots
boxplot(percent_confirmed~NAME, data=confirmed_df, main="Boxplot of growth by states")
boxplot(percent_deaths~NAME, data=death_df, main="Boxplot of deaths by stae")
```
This did not generate any satisfactory or usable results as there were too many states to make sense of anything so I decided to see if I could get a better results via aggrgation and kmeans
```{r}
# aggregate data
state_covid_growth <- aggregate(list(last_week_confirmed['prev_c'], last_week_confirmed['total_c']), by=last_week_confirmed['NAME'], sum)
state_death <- aggregate(list(last_week_deaths['prev_d'], last_week_deaths['total_d']), by=last_week_deaths['NAME'], sum)

# recalculate aggregatd percentages
state_death$state_death <- (state_death$total_d - state_death$prev_d)
state_covid_growth$state_covid_growth <- (state_covid_growth$total_c - state_covid_growth$prev_c)
state_death$percent_deaths <- (state_death$state_death / state_death$total_d)
state_covid_growth$percent_confirmed <- (state_covid_growth$state_covid_growth / state_covid_growth$total_c)

# Once again remove states with NaN
state_death <- state_death[complete.cases(state_death), ]
state_covid_growth <- state_covid_growth[complete.cases(state_covid_growth), ]

# Make a matrix for kmeans
cov_death_matrix <- data.matrix(state_covid_growth)
cov_growth_matrix <- data.matrix(state_death)

# Only use the percentage data
cov_death_matrix <- cov_death_matrix[,c(5)]
cov_growth_matrix <- cov_growth_matrix[,c(5)]


# Check number of kmeans (I will likely still use 5 as I want "grades")
wssplot <- function(data, nc=15, seed=10){
  wss <- data.frame(cluster=1:nc, quality=c(0))
  for (i in 1:nc){
    set.seed(seed)
    wss[i,2] <- sum(kmeans(data, centers=i)$withinss)}
  ggplot(data=wss,aes(x=cluster,y=quality)) + 
    geom_line() + 
    ggtitle("Quality of k-means by Cluster")
}
wssplot(cov_death_matrix, 25, 300)
wssplot(cov_growth_matrix, 25, 300)

# Do kmeans
head(cov_growth_matrix)
cnf_km <- kmeans(cov_growth_matrix, centers = 5)
cnf_kclass=data.frame(as.factor(cnf_km$cluster))
kmeans_cnf <-data.frame(cbindX(state_covid_growth, cnf_kclass))

# Fuse Kclass with matrixes
dth_km <- kmeans(cov_death_matrix, centers = 5)
dth_kclass=data.frame(as.factor(dth_km$cluster))
kmeans_dth <-data.frame(cbindX(state_death, dth_kclass))

kmeans_dth <- kmeans_dth[complete.cases(kmeans_dth), ]
kmeans_cnf <- kmeans_cnf[complete.cases(kmeans_cnf), ]

names(kmeans_cnf)[6] <- 'cnf_kclass'
names(kmeans_dth)[6] <- 'dth_kclass'

# plot results
ggplot(kmeans_cnf) +
  geom_density(aes(x = percent_confirmed, color = cnf_kclass)) +
  ggtitle("Distribution of confirmed growth by kclass")

ggplot(kmeans_dth) +
  geom_density(aes(x = percent_deaths, color = dth_kclass)) +
  ggtitle("Distribution of death growth by kclass")
```

These plots also weren't very promising.  The kmeans didn't form into 5 nice humps.  This may be due to the fact that I didn't use the elbow method, but either way I decided to not use kmeans and isntead use a map for grading.

I started doing this by loading in the Map Data and then combining it with the other data I had

```{r}
# Load in map data
states.shapes <- readRDS("../data/json/us_projection.Rds")
states <- states.shapes
states <- data.frame(states)
states <- states[c("fips_state", "name")]
colnames(states) <- c("FIPS", "NAME")

#combine map data, as well as new case and death data
map_data <- merge(states, state_covid_growth, by="NAME", type="inner")
map_data <- merge(map_data, state_death, by="NAME", type="inner")
map_data = map_data[match(states$NAME, map_data$NAME),]

```

Finally I made the maps

```{r}
colors <- c("grey","#426C85","#67a9cf","#d1e5f0","#f7f7f7","#fddbc7","#ef8a62","#b2182b")
    bins <- c(0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 1,-Inf)
    pal2 <- leaflet::colorBin(colors, domain = map_data$percent_confirmed, bins = bins, reverse=FALSE)
    labels2 <- sprintf(
      "<strong>%s</strong><br/>
      Percent of new COVID cases from last week: %.2g",
      map_data$NAME, map_data$percent_confirmed
    ) %>% lapply(htmltools::HTML)
    
    leaflet(states.shapes) %>%
      setView(-96, 37.8, 4) %>% 
      addPolygons(
        fillColor = ~pal2(map_data$percent_confirmed),
        weight = 2,
        opacity = 1,
        color = "white",
        dashArray = "3",
        fillOpacity = 0.7,
        highlight = highlightOptions(
          weight = 5,
          color = "#666",
          dashArray = "",
          fillOpacity = 0.7,
          bringToFront = TRUE),
        label = labels2,
        labelOptions = labelOptions(
          style = list("font-weight" = "normal", padding = "3px 8px"),
          textsize = "15px",
          direction = "auto")) %>% 
      addLegend(pal = pal2, 
                values = ~map_data$percent_confirmed, 
                opacity = 0.7, 
                title = "Percent of total COVID<br>cases from previous week",
                position = "bottomright"
                ) %>%
      addProviderTiles("MapBox", options = providerTileOptions(
        id = "mapbox.light",
        accessToken = Sys.getenv('MAPBOX_ACCESS_TOKEN')))

colors <- c("grey","#426C85","#67a9cf","#d1e5f0","#f7f7f7","#fddbc7","#ef8a62","#b2182b")
    bins <- c(0, 0.075, 0.125, 0.15, 0.225, 0.275, 0.35, 1,-Inf)
    pal2 <- leaflet::colorBin(colors, domain = map_data$percent_deaths, bins = bins, reverse=FALSE)
    labels2 <- sprintf(
      "<strong>%s</strong><br/>
      Percent COVID-19 deaths in last week %.2g",
      map_data$NAME, map_data$percent_deaths
    ) %>% lapply(htmltools::HTML)
    
    leaflet(states.shapes) %>%
      setView(-96, 37.8, 4) %>% 
      addPolygons(
        fillColor = ~pal2(map_data$percent_deaths),
        weight = 2,
        opacity = 1,
        color = "white",
        dashArray = "3",
        fillOpacity = 0.7,
        highlight = highlightOptions(
          weight = 5,
          color = "#666",
          dashArray = "",
          fillOpacity = 0.7,
          bringToFront = TRUE),
        label = labels2,
        labelOptions = labelOptions(
          style = list("font-weight" = "normal", padding = "3px 8px"),
          textsize = "15px",
          direction = "auto")) %>% 
      addLegend(pal = pal2, 
                values = ~map_data$percent_deaths, 
                opacity = 0.7, 
                title = "Percent of total COVID<br>deaths from last week",
                position = "bottomright"
                ) %>%
      addProviderTiles("MapBox", options = providerTileOptions(
        id = "mapbox.light",
        accessToken = Sys.getenv('MAPBOX_ACCESS_TOKEN')))
```
I took some time to adjust the values of each color in order to get a nice and relatively even distribution.

### Results

The maps I created were quite sucessful.  They show which states are controlling their outbreaks and which ones are getting worse.  New York for example is the epicenter of the outbreak; however the growth of new cases and deaths is fairly low.  3,000 new cases isn't that mahy when you have over 250,000 cases already.  This clearly shows New York doing well.  Other states like Minnesota and South Dakota are not doing as well at controlling their outbreaks, even though they aren't as far along into the cycle.  Meanwhile states like Vermont and Montana also are not very far into their outbreaks but are still keeping things under control.

### Discussion

The maps are a wondrful addition in my opinion.  Each map could be a seperate grade that goes into the "report" card.  I still was slightly disapointed that I was unable to find a way of working features into the report.  That said, it fills all of the other goals I set out for myself.  Population and past sucess isn't important (as seen in New York).  Instead what is curretnly happening is the most important.  There is a slight issue with low number states, as small increases can look much bigger in comparison (like South Dakota); however, I'm not fully convinced this is a bad thing, and it is better than bias against a large state.  Moreover, statistical error caused by a small n wouldn't be avoided in other methods.


# Summary and COVIDMINDER Recommendations

* Overall, what insights did you find  about the  COVID-19 epidemic in your analysis?    

I found that rates of features associated with personal survival were not strongly correlated with statewide sucess or failure.  I found that this made creating a report card for a state difficult, but I was still able to create a model that showed how well a state was doing recently with other data.

* What recommendations do you have for COVIDMINDER for  Data utilization, Analytics, Visualizations, User interface design, etc.

I would add a report card to COVIDMINDER; however, when doing so I would test that the features going into that report card actually relate to COVID sucess.  Finally if you are unable to create a report card based on features you may be able to make one based on the growth of COVID in the state (to see if the state is effectively combating COVID).

# References

https://www.cdc.gov/coronavirus/2019-ncov/need-extra-precautions/people-at-higher-risk.html

https://www.cdc.gov/coronavirus/2019-ncov/index.html

https://www.cdc.gov/coronavirus/2019-ncov/faq.html

https://www.cdc.gov/coronavirus/2019-ncov/communication/guidance-list.html?Sort=Date%3A%3Adesc